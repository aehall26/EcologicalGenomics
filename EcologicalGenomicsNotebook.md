# Title

## Author: Alison Hall
### Affiliation:
### E-mail contact: alison.hall@uvm.edu

### Start Date: 2020-01-13
### End Date: 2020-05-08
### Project Descriptions:   Ecological Genomics





# Table of Contents:
* [Entry 1: 2020-01-13, Monday](#id-)
* [Entry 2: 2020-01-14, Tuesday](#id-section2)
* [Entry 3: 2020-01-15, Wednesday](#id-section3)
* [Entry 4: 2020-01-16, Thursday](#id-section4)
* [Entry 5: 2020-01-17, Friday](#id-section5)
* [Entry 6: 2020-01-20, Monday](#id-section6)
* [Entry 7: 2020-01-21, Tuesday](#id-section7)
* [Entry 8: 2020-01-22, Wednesday](#id-Command Line)
* [Entry 9: 2020-01-23, Thursday](#id-section9)
* [Entry 10: 2020-01-24, Friday](#id-section10)
* [Entry 11: 2020-01-27, Monday](#id-section11)
* [Entry 12: 2020-01-28, Tuesday](#id-section12)
* [Entry 13: 2020-01-29, Wednesday Population Genomics Day 1](#id-section13)
* [Entry 14: 2020-01-30, Thursday](#id-section14)
* [Entry 15: 2020-01-31, Friday](#id-section15)
* [Entry 16: 2020-02-03, Monday](#id-section16)
* [Entry 17: 2020-02-04, Tuesday](#id-section17)
* [Entry 18: 2020-02-05, Wednesday](#id-Population Genetics Day2)
* [Entry 19: 2020-02-06, Thursday](#id-section19)
* [Entry 20: 2020-02-07, Friday](#id-section20)
* [Entry 21: 2020-02-10, Monday](#id-section21)
* [Entry 22: 2020-02-11, Tuesday](#id-section22)
* [Entry 23: 2020-02-12, Wednesday](#id-section23)
* [Entry 24: 2020-02-13, Thursday](#id-section24)
* [Entry 25: 2020-02-14, Friday](#id-section25)
* [Entry 26: 2020-02-17, Monday](#id-section26)
* [Entry 27: 2020-02-18, Tuesday](#id-section27)
* [Entry 28: 2020-02-19, Wednesday](#id-section28)
* [Entry 29: 2020-02-20, Thursday](#id-section29)
* [Entry 30: 2020-02-21, Friday](#id-section30)
* [Entry 31: 2020-02-24, Monday](#id-section31)
* [Entry 32: 2020-02-25, Tuesday](#id-section32)
* [Entry 33: 2020-02-26, Wednesday](#id-section33)
* [Entry 34: 2020-02-27, Thursday](#id-section34)
* [Entry 35: 2020-02-28, Friday](#id-section35)
* [Entry 36: 2020-03-02, Monday](#id-section36)
* [Entry 37: 2020-03-03, Tuesday](#id-section37)
* [Entry 38: 2020-03-04, Wednesday](#id-section38)
* [Entry 39: 2020-03-05, Thursday](#id-section39)
* [Entry 40: 2020-03-06, Friday](#id-section40)
* [Entry 41: 2020-03-09, Monday](#id-section41)
* [Entry 42: 2020-03-10, Tuesday](#id-section42)
* [Entry 43: 2020-03-11, Wednesday](#id-section43)
* [Entry 44: 2020-03-12, Thursday](#id-section44)
* [Entry 45: 2020-03-13, Friday](#id-section45)
* [Entry 46: 2020-03-16, Monday](#id-section46)
* [Entry 47: 2020-03-17, Tuesday](#id-section47)
* [Entry 48: 2020-03-18, Wednesday](#id-section48)
* [Entry 49: 2020-03-19, Thursday](#id-section49)
* [Entry 50: 2020-03-20, Friday](#id-section50)
* [Entry 51: 2020-03-23, Monday](#id-section51)
* [Entry 52: 2020-03-24, Tuesday](#id-section52)
* [Entry 53: 2020-03-25, Wednesday](#id-section53)
* [Entry 54: 2020-03-26, Thursday](#id-section54)
* [Entry 55: 2020-03-27, Friday](#id-section55)
* [Entry 56: 2020-03-30, Monday](#id-section56)
* [Entry 57: 2020-03-31, Tuesday](#id-section57)
* [Entry 58: 2020-04-01, Wednesday](#id-section58)
* [Entry 59: 2020-04-02, Thursday](#id-section59)
* [Entry 60: 2020-04-03, Friday](#id-section60)
* [Entry 61: 2020-04-06, Monday](#id-section61)
* [Entry 62: 2020-04-07, Tuesday](#id-section62)
* [Entry 63: 2020-04-08, Wednesday](#id-section63)
* [Entry 64: 2020-04-09, Thursday](#id-section64)
* [Entry 65: 2020-04-10, Friday](#id-section65)
* [Entry 66: 2020-04-13, Monday](#id-section66)
* [Entry 67: 2020-04-14, Tuesday](#id-section67)
* [Entry 68: 2020-04-15, Wednesday](#id-section68)
* [Entry 69: 2020-04-16, Thursday](#id-section69)
* [Entry 70: 2020-04-17, Friday](#id-section70)
* [Entry 71: 2020-04-20, Monday](#id-section71)
* [Entry 72: 2020-04-21, Tuesday](#id-section72)
* [Entry 73: 2020-04-22, Wednesday](#id-section73)
* [Entry 74: 2020-04-23, Thursday](#id-section74)
* [Entry 75: 2020-04-24, Friday](#id-section75)
* [Entry 76: 2020-04-27, Monday](#id-section76)
* [Entry 77: 2020-04-28, Tuesday](#id-section77)
* [Entry 78: 2020-04-29, Wednesday](#id-section78)
* [Entry 79: 2020-04-30, Thursday](#id-section79)
* [Entry 80: 2020-05-01, Friday](#id-section80)
* [Entry 81: 2020-05-04, Monday](#id-section81)
* [Entry 82: 2020-05-05, Tuesday](#id-section82)
* [Entry 83: 2020-05-06, Wednesday](#id-section83)
* [Entry 84: 2020-05-07, Thursday](#id-section84)
* [Entry 85: 2020-05-08, Friday](#id-section85)

------
<div id='id-section1'/>
### Entry 1: 2020-01-13, Monday.



------
<div id='id-section2'/>
### Entry 2: 2020-01-14, Tuesday.



------
<div id='id-section3'/>
### Entry 3: 2020-01-15, Wednesday.







------
<div id='id-section4'/>
### Entry 4: 2020-01-16, Thursday.



------
<div id='id-section5'/>
### Entry 5: 2020-01-17, Friday.



------
<div id='id-section6'/>
### Entry 6: 2020-01-20, Monday.



------
<div id='id-section7'/>
### Entry 7: 2020-01-21, Tuesday.



------
<div id='id-section8'/>
### Entry 8: 2020-01-22, Wednesday.

# Introduction to connecting to the Unix server bash command line, and our data set

## Using the Class Server
  + we will use a server called pbio381 to store our very large data sets
  + we can connect to it via Unix using a secure shell protocol while on campus
  + to do this, I would type in git bash the following:
    + ssh aehall@pbio381.uvm.edu
      + doing this will prompt us to enter our UVM net ID password

## Basic Commands
  +  `pwd` allows you to see the path to your current directory type
    + you can use this to copy and paste your directory when you need it for
    reference while coding
  +  `cd` refers to changing directory use this command when you want
      to change your working directory. You can use it to start typing a directory
      you want to move into then press tab to complete.
    + my folder that is connected to my github can be entered by typing this:
      + `$ cd Documents/UVM/EcologicalGenomics/EcologicalGenomics`
      + to back up to just the first EcologicalGenomics folder you can enter `cd ..
  + `ll` allows you to view contents of a folder
  + `mkdir` creates a new folder
    + to name this folder aehfiles I would type `mkdir aehfiles`
    + if I did this while in the second EcologicalGenomics folder that's where
    the folder called aehfiles would live
  + `head n- # filename` will show you the first # lines of Data
  + `tail -n #filename` shows the # last lines of Data
  + `mv document.txt AlisonWork/Example` would move a document called document
  into the folder Example which lives in the folder AlisonWork
  + `rm document.txt` removes that file

## Red Spruce Data Set (*Picea rubens*)
  + the data set is part of an NSF project on population genomics of climate adaptation
  + over the course of the semester we will analyze exome-capture illumina data
  + data were collected by the Keller Lab from across the Appalachian Mountains
  ranging from Tennessee, USA to New Brunswick, CA.
  + Region- refers to the geographical and genetic cluster each ind. belongs to
    + Core(C), Margin (M), or Edge(E)
    + we will just focus on the *Edge*

## Data Storage:
      + `cd /data/project_data/RS_ExomeSeq`
      + `fastq` contains paired end illumina seq files
      + `metadata` folder has the `RS_Exome_metadata.txt` file
        + we won't edit this, because editing this will share our edits with the group
        + copy this folder and put it in your `mydata` (which we created to exist my EcologicalGenomics folder)
        + `cp RS_ExomeSeq.txt ~/<EcologicalGenomics>/mydata/`
      + use grep to select just the edge populations and move them to a new folder
        + do this from within the my data folder
          + `grep -w "E" RS_Exome_metadata.txt >Edge_only.txt`
            + grep- searches for data that match a term of interest
            + -w tells grep what that term of interest is. In this case only terms that match E as-is
            + `>` pipes to a new text file called Edge_only
            + learn more about grep by typing `mangrep` which will show you the manual

## Editing files in Unix
  + `vim .bash` to edit a bash file
  + `i` to insert into this file
  + click esc to leave edit mode
  + `:wq` to save changes and quit

## Git Command Line Push and Pull
  + `git status`
  + `git add --all` or `git -A`
  + `git status` again to make sure all previously red text documents are in green
  + `git commit -m"type message here within quotes"`
  + `git push` to push to online repo









------
<div id='id-section9'/>
### Entry 9: 2020-01-23, Thursday.



------
<div id='id-section10'/>
### Entry 10: 2020-01-24, Friday.



------
<div id='id-section11'/>
### Entry 11: 2020-01-27, Monday.



------
<div id='id-section12'/>
### Entry 12: 2020-01-28, Tuesday.



------
<div id='id-section13'/>
### Entry 13: 2020-01-29, Wednesday.

# [Population Genomics Day 1](https://pespenilab.github.io/Ecological-Genomics/2020-01-29_PopGenomics_Day1.html)


## More about Red Spruce
  + coniferous tree that plays a prominent role in montane communities throughout the Appalachians. Thrives in cool, moist climates of the high elevation mountains
  + "island" populations exit on mountaintops are in low latitude trailing edge of the range spanning from Maryland to Tennessee
  + these are remnant populations of spruce. The trees retreated to the mountain top refugia as the climate warmed.
  + this caused them to be highly isolated from other stands
  + red spruce are being replanted to conserve the population, but these individuals aren't necessarily the ones most adapted to current climate conditions
  + out project aims to understand the genetic resource represented by these fragmented edge populations. We will use exome capture data to study where the alleles are located that are selected upon. Alleles that are selected upon are the alleles that we need to conserve.
  + Main goals of this project in the Keller Lab
    1. characterize genetic diversity and pop structure across range
    2. identify regions of the genome that show evidence of positive selection in response to climate gradients
    3. map the genetic basis of climate adaptive phenotypes
  + Using this information we can inform areas of the range that are most likely to experience climate maladaptation and help guide mitigation strategies.


## Basic Coding Pipeline
  1. Visualize using FastQC. This is a quality control tool for high throughput sequence data.
  It is used on raw sequence data and provides a modular set of analyses that gives a quick impression on any large problems with your data. It can be used to uncover biases in data that can affect how you could use it.
  Unlike the QC report provided by a sequencer, FastQC's report spots problems that originate either in the sequencer or in the starting library material.
  2. Clean the data with Trimmomatic. Trimmomatic performs trimming for the illumina specific sequences, and uses a sliding window approach to trim. It travels along the tread and cuts where quality falls below a certain specified threshold. It will cut the reads to a specific length and works with FastQC
    + cleaned reads come out in `.fastq.gz`. Don't open this file on the computer! This is the file we can use to visualize the FastQC
  3. After this you'll visualize again with FastQC
  4. Then calculate the numbers of cleaned, high quality reads that go into mapping
  5. Align these cleaned reads from each sample to the reference assembly to generate sequence alignment file using bwa. Files go in as .fastqc and out as .sam.
    +alignment post processing: we use samtools, sambamba whcih converts alignment into binary form (.bam)
  6. Remove PCR duplicates and calculate alignment statistics (% reads mapped successfully, mapping quality scores, and average depth of coverage per individual)

*Note: I used the population XWS for my analysis*
### New code learned today:
+ `chmod u+x file.sh` gives user permission to make script executable.
+ `./fastqc.sh` runs the fastqc script
+ `zcat` like `head` for compressed files. FastQC files are big, so big that they must be compressed we can't use `head` on a compressed file.
  to peek inside a compressed file while keeping it compressed use the `zcat`
  + example of how I used this:
    + data are located here: `cd /data/project_data/RS_ExomeSeq/fastq/edge_fastq`
      + `zcat XWS_05_R1_fastq.gz | head -n 4` to see the first 4 lines of my files
      + the output of this is

        ```@GWNJ-0842:368:GW1809211440:2:1101:13667:2206 1:N:0:NGGTGGTA+GTCAATACTCCAAACATTTTCAAGCGCTTCAGCTTCTACACTTAATCTAGGGGTTAACCATGAAGATTGTGTGCTTAATGAAGTAAGTTTAGATATCTCATGTGGAAGCATCTATAAGTTCAAACATCCATTTAAATTAAGATAATTTAGATTTTTAA+AA-AA<J--FF-FFJJFFJJJAFFJJAJJAF<JJJAAF7FFJJJ<JJJFJJJJJJJJ7AJ-<FJ<-FFJ7FJ<7FFAFJJJJFAF-<JFFFJFJ--7F7AJAJF-AFJJJJJJ7FFF-AF7JFF<FFJJJA<AAJJJJ-7AA<FJ7JJJ```

        can be understood as follows:
          + First line- will start with an @ sign then identification information about the read, including info about the computer that it came from and barcodes that ID the sequence
          + Second line- the DNA sequence read. N represents any base. Our read was 150bp so that's how many we should expect
          + Third line- separates the second line (sequence) from the 4th (score) and starts with a +
          + Fourth line- is the code that shows us how confident we are that we read the line correctly. It is comprised of a string of characters which represents the quality scores.
            + "Phred quality score"- high quality scores have a higher chance that the machine correctly called the nucleotide (ex: quality score of 2 is 20% chance of the read being incorrect, a phred score of 10 means that the base call accuracy is 90%, probability of an incorrect base call is 1 in 10).
            + what does my phred score look like?

Below is my code that I used to process all the files belonging to XWS population located in the edge regions:



  ``````
  !/bin/bash
  #this directory is using bash code

  cd ~/EcologicalGenomics/myresults/ #specifies where I want this to be stored

  mkdir fastqc #I am creating a new directory within myresults

  for file in data /data/project_data/RS_ExomeSeq/fastq/edge_fastq/XWS*fastq.gz

  do #do this  for each file in that folder that matches my population

  fastqc ${file} -o fastqc/

  #the lower case o means dont spit it to this directory put it in the folder i tell you to be in. only need to give the portion of the path that is beyond where you are. Meaning for this, you should be working within the myresults folder when you carry this out

  done
``````
### Cleaning the reads for each of these files using Trimmomatic program
+ this program is installed on the server
+ there's an example script in `/data/scripts` which I copied into myscripts folder and opened and edited using vim
+ run the trimmomatic program on the files in the XWA population

  ```
  cp /data/scripts/trim_loop.sh  ~/myrepo/myscripts/ # copies the script to your home scripts dir
  vim trim_loop.sh    # open the script with vim to edit
  ```
+ Here's the edited version of the script that I used:

  ```
  #!/bin/bash   

  cd /data/project_data/RS_ExomeSeq/fastq/edge_fastq  

  for R1 in XWS*R1_fastq.gz  

  do

  	R2=${R1/_R1_fastq.gz/_R2_fastq.gz}
  	f=${R1/_R1_fastq.gz/}
  	name=`basename ${f}`

  	java -classpath /data/popgen/Trimmomatic-0.33/trimmomatic-0.33.jar org.usadellab.trimmomatic.TrimmomaticPE \
          -threads 1 \
          -phred33 \
           "$R1" \
           "$R2" \
           /data/project_data/RS_ExomeSeq/fastq/edge_fastq/pairedcleanreads/${name}_R1.cl.pd.fq \
           /data/project_data/RS_ExomeSeq/fastq/edge_fastq/unpairedcleanreads/${name}_R1.cl.un.fq \
           /data/project_data/RS_ExomeSeq/fastq/edge_fastq/pairedcleanreads/${name}_R2.cl.pd.fq \
           /data/project_data/RS_ExomeSeq/fastq/edge_fastq/unpairedcleanreads/${name}_R2.cl.un.fq \
          ILLUMINACLIP:/data/popgen/Trimmomatic-0.33/adapters/TruSeq3-PE.fa:2:30:10 \
          LEADING:20 \
          TRAILING:20 \
          SLIDINGWINDOW:6:20 \
          MINLEN:35

  done
  ```


+ these are the areas that trimmomatic trims

  ```
  ILLUMINACLIP: Cut adapter and other illumina-specific sequences from the read.
  LEADING: Cut bases off the start of a read, if below a threshold quality
  TRAILING: Cut bases off the end of a read, if below a threshold quality
  SLIDINGWINDOW: Perform a sliding window trimming, cutting once the average quality within the window falls below a threshold.
  MINLEN: Drop the read if it is below a specified length
  ```




------
<div id='id-section14'/>
### Entry 14: 2020-01-30, Thursday.

------
<div id='id-section15'/>
### Entry 15: 2020-01-31, Friday.



------
<div id='id-section16'/>
### Entry 16: 2020-02-03, Monday.



------
<div id='id-section17'/>
### Entry 17: 2020-02-04, Tuesday.



------
<div id='Population Genetics Day2'/>
### Entry 18: 2020-02-05, Wednesday.

# [Population Genetics Day 2](https://pespenilab.github.io/Ecological-Genomics/2020-02-05_PopGenomics_Day2.html)
+ Today we are going to take our trimmed paired reads and map them against the reference genome. We only map the paired reads because the unpaied ones don't have a match that passed the quality score test
  + because of this trimming process, our data are more high quality
+ we are going to remove PCR duplicates
+ Use BWA to map to a reference genome
  + a sam file can be looked at
  + bam file is binary but better for Storage
  + the genome size of the Norway spruce *Picean abies* is too big! ~20gb so using our probes we reduced the size of this
    + We did this using a BLAST search of each probe.
  + we have information on contigs, which is a small sequence of DNA used to contruct a map. At this stage, we don't know where the contigs lie on the chromosome.
  + N50take the biggest contig and go until you reach 1/2 of the total Mb (334Mb) of the entire genome. N50 is the last contig in that. This means that 50% of th genome comes *before* that contig.
    + In our case this contig happens to be 101,375bp in length
    + you want this number to be *large* because you don't know where the contigs are on the chromosome, so it's better to have large contigs making up the majority of the chromosome- gives you more detailed spatial information that is closer to the actual chromosomes.

`Ctrl C` kills your code. May have to repeat yourself.
 `screen bash mypipeline.sh` to run mypipeline script that wraps all the other scripts

 `Ctrl A` then `Ctrl D` to detach

 ## Reference genome:

 + we have it here on the server: `cd /data/project_data/RS_ExomeSeq/ReferenceGenomes/`

 + if we didn't could get it online:
`wget "http://plantgenie.org/Picea_abies/v1.0/FASTA/GenomeAssemblies/Pabies1.0-genome.fa.gz"`

## Mapping

Here's the code I used:
```
#!/bin/bash

# this script calls the R1 and R2 reads for each individual in the population
# bwa maps reads to the reference genome
# options are preceded by a - (see tutorial for specific description of the code)
ref="/data/project_data/RS_ExomeSeq/ReferenceGenomes/Pabies1.0-genome_reduced.fa"
# Write a loop to map each individual within my population
for forward in ${input}*_R1.cl.pd.fq
#by including the wild card it says do it for every member within my population that matches my code and ends with cleaned paired .fq
do
	reverse=${forward/_R1.cl.pd.fq/_R2.cl.pd.fq} #initialize on the forward read and then take the name of the forward read and substitute to make the name for the reverse read
	f=${forward/_R1.cl.pd.fq/} #delete the extension
	name=`basename ${f}` #leave us with just the name of the population. basename is a function within bash. ` means you're giving a command within what youre definin g
	bwa mem -t 1 -M ${ref} ${forward} ${reverse} > ${output}/BWA/${name}.sam
done
```

## process_bam.sh
+ this code converts our sam files to bam files and sorts them.
+ it also gets rid of PCR duplicate sequences
  + helpful [paper](https://bmcbioinformatics.biomedcentral.com/articles/10.1186/s12859-016-1097-3#Sec1) discussing this
  + PCR duplicates occur when multiple reads align to the same start position in the genome
  + PCR duplicates should be removed to reduce the chance that we will misidentify a duplicate as a true variation in the chromosome.
  + SAMTools identifies PCR duplicates and keeps the read with the highest mapping quality score
  + removing duplicates can improve accuracy of variant calls

 ```
 #!/bin/bash
 # this is where our output sam files are goin gto get converted into binary format (bam)
 # Then were going to sort the bam files, remove the PCR duplicates, and index the duplicates

 #first let's convert sam to bam and then we sort

 #for f in ${output}/BWA/${mypop}*.sam

 #do
 	#out=${f/.sam}
 	#substitute the sam because we'll change it out for a .bam
 	#sambamba-0.7.1-linux-static view -S --format=bam ${f} -o ${out}.bam #this is specific to our server -S means its a sam file. then we'll format this into a bam file
 	#samtools sort ${out}.bam -o ${out}.sorted.bam #output into a sorted.bam file
 #done

 #now let's remove the PCR duplicates beause we dont want the pseudo repolication that those represent

 for file in ${output}/BWA/${mypop}*.sorted.bam

 do
 	f=${file/.sorted.bam/}
 	sambamba-0.7.1-linux-static markdup -r -t 1 ${file} ${f}.sorted.rmdup.bam
 	#file is defined above. #marks the PCR duplicates and removes them.
 	# Assumes it's a dupiicate if you have more than one read with the exact same start and end place.
 done

 #now to finish, we'll index our files. index creates coordinates on the file which allow for quick look up and retrieval

 for file in ${output}/BWA/${mypop}*.sorted.rmdup.bam

 do
 	samtools index ${file}
 done
 ```

## bamstats.sh
 + works similarly to the other script but now it runs after the PCR duplicates were removed.

 ```
 #!/bin/bash
 #set repo
 myrepo="/users/a/e/aehall/EcologicalGenomics"

 mypop="XWS"
 output="/data/project_data/RS_ExomeSeq/mapping"

 echo "Num.reads R1 R2 Paired MateMapped Singletons MateMappedDiffChr" >${myrepo}/myresults/${mypop}.flagstats.txt
 #name of the file mypopflagstats.txt lives in the myresults folder in my repo

 for file in ${output}/BWA/${mypop}*sorted.rmdup.bam
 	do
 		f=${file/.sorted.rmdup.bam/}
 		name=`basename ${f}` #base name is a function and helps us rename these files
 		echo ${name} >> ${myrepo}/myresults/${mypop}.names.txt #store individual sample names
 		#double arrow allows us to keep growing a single carrot would make it write it over and over again
 		samtools flagstat ${file} | awk 'NR>=6&&NR<=12 {print $1}' | column -x
 	done >>${myrepo}/myresults/${mypop}.flagstats.txt
 		#pipe means take these results and pass through to next command.
 		#awk strips just the rows of data we care about.
 		#NR is number of rows greater than or equal to six and less than or equal to 12
 # calculate depth of coverage from our bam files

 for file in ${output}/BWA/${mypop}*sorted.rmdup.bam

 	do
 		samtools depth ${file} | awk '{sum+=3} END {print sum/NR}'
 		#take a rolling sum of the number in the third column
 	done >> ${myrepo}/myresults/${mypop}.coverage.txt
 ```

## Wrap these scripts into a pipeline
 + this will run the scripts we just created
 + Q: why is bamstats left out?

 ```
 #!/bin/bash
 # We'll use this as a wrapper to run our different mapping scripts
 myrepo="/users/a/e/aehall/EcologicalGenomics"
 # My population:
 mypop="XWS"
 # Directory to our cleaned and paired reads:
 input="/data/project_data/RS_ExomeSeq/fastq/edge_fastq/pairedcleanreads/${mypop}"
 # define input as JUST cleaned paired reads that have my population in it
 #directory to store our outputs
 output="/data/project_data/RS_ExomeSeq/mapping"
 # run mapping.sh

 #to do it in a way that allows these variables to get passed into the script that's being run use the command source, then the name of the file
 #source ./mapping.sh
 source ./process_bam.sh
```









------
<div id='id-section19'/>
### Entry 19: 2020-02-06, Thursday.



------
<div id='id-section20'/>
### Entry 20: 2020-02-07, Friday.



------
<div id='id-section21'/>
### Entry 21: 2020-02-10, Monday.



------
<div id='id-section22'/>
### Entry 22: 2020-02-11, Tuesday.



------
<div id='id-section23'/>
### Entry 23: 2020-02-12, Wednesday.

# [Population Genomics Day 3](https://pespenilab.github.io/Ecological-Genomics/2020-02-12_PopGenomics_Day3.html)

## Look at sam files
+ `tail -n 100 XWS_05.sam`
+ this is a text file tjat has infomation about the alignment of reads in a FastQC file to a reference genome.
+ One SAM file exists for each FastQC file
  + there are two numbers: one of the left is a contig, the number of the right is the length
  + there is a row of data and one of the potential reads is mapped somewhere on the genome
  + not everything is mapped (as expected)
+ GWNJ- name of file
+ after that comes a number that gives information on how a read behaves that can be decoded
  + there's a decoder for this info linked in the tutorial
  + my flag is 163 which using the decoder I see that means that
+ then comes the contig that that specific read mapped to
+ then comes the leftmost read
+ then a quality score base

## Stats on sam Files
+ to get a summary of how well our reads mapped to the reference we use the program SAMTools
+ `samtools flagstat XWS_05.sam`

## bam_stats.sh
+ this code uses the program samtools' flagstat command to get read counts after mapping and the depth command to get the depth of coverage meaning how many reads cover each mapped position (on average)
+ the code I used to do this is below

```
#!/bin/bash
#set repo
myrepo="/users/a/e/aehall/EcologicalGenomics"
mypop="XWS"
output="/data/project_data/RS_ExomeSeq/mapping"
echo "Num.reads R1 R2 Paired MateMapped Singletons MateMappedDiffChr" >${myrepo}/myresults/${mypop}.flagstats.txt
#name of the file mypopflagstats.txt lives in the myresults folder in my repo
for file in ${output}/BWA/${mypop}*sorted.rmdup.bam
	do
		f=${file/.sorted.rmdup.bam/}
		name=`basename ${f}` #base name is a function and helps us rename these files
		echo ${name} >> ${myrepo}/myresults/${mypop}.names.txt #store individual sample names
		#double arrow allows us to keep growing a single carrot would make it write it over and over again
		samtools flagstat ${file} | awk 'NR>=6&&NR<=12 {print $1}' | column -x
	done >>${myrepo}/myresults/${mypop}.flagstats.txt
		#pipe means take these results and pass through to next command.
		#awk strips just the rows of data we care about.
		#NR is number of rows greater than or equal to six and less than or equal to 12
# calculate depth of coverage from our bam files
for file in ${output}/BWA/${mypop}*sorted.rmdup.bam
  do
		samtools depth ${file} | awk '{sum+=3} END {print sum/NR}'
		#take a rolling sum of the number in the third column
	done >> ${myrepo}/myresults/${mypop}.coverage.txt
```		 
## ANGSD- Analysis of Next Generation Sequence Data
+ we should not assume that what we are observing is the true geneotype, there is some degree of uncertainty with this.
+ read data are counts that produce a multinomial distribution of alleles at a site. With only a few reads, you can't determine the geneotype confidently
  + geneotype likelihood is the probability of observing the sequence data (reads that contain a particular base) given the geneotype at that site
  + diversity statistics model this probability
+ ANGSD allows us to incorporate uncertainty into our analysis
+ code for my ANGSD and calculating the SFS
  + see Steve's tutorial for better detail on the code explanation  

```
myrepo="/users/a/e/aehall/EcologicalGenomics"


mkdir ${myrepo}/myresults/ANGSD

output="${myrepo}/myresults/ANGSD"

mypop="XWS"

ls /data/project_data/RS_ExomeSeq/mapping/BWA/${mypop}*sorted.rm*.bam >${output}/${mypop}_bam.list

REF="/data/project_data/RS_ExomeSeq/ReferenceGenomes/Pabies1.0-genome_reduced.fa"

# Estimating GL's and allele frequencies for all sites with ANGSD

ANGSD -b ${output}/${mypop}_bam.list \
-ref ${REF} -anc ${REF} \
-out ${output}/${mypop}_allsites \
-nThreads 1 \
-remove_bads 1 \
-C 50 \
-baq 1 \
-minMapQ 20 \
-minQ 20 \
-setMinDepth 3 \
-minInd 2 \
-setMinDepthInd 1 \
-setMaxDepthInd 17 \ #could me PCR duplicates
-skipTriallelic 1 \ #probably an error because youre unlikely to see more mutations at one location
-GL 1 \ #geneotype likelihoods
-doCounts 1 \ #generate counts at each site
-doMajorMinor 1 \ #majorminor status equivalent across all sites. major is often ancestral allele. minor alleles are rare and often derived
-doMaf 1 \ #major minor frequencies
-doSaf 1 \
-doHWE 1 \ #test for HW equilibrium to see that anything is majorly outside
# -SNP_pval 1e-6

myrepo="/users/a/e/aehall/EcologicalGenomics"


mkdir ${myrepo}/myresults/ANGSD

output="${myrepo}/myresults/ANGSD"

mypop="XWS"

ls /data/project_data/RS_ExomeSeq/mapping/BWA/${mypop}*sorted.rm*.bam >${output}/${mypop}_bam.list

REF="/data/project_data/RS_ExomeSeq/ReferenceGenomes/Pabies1.0-genome_reduced.fa"

# Estimating GL's and allele frequencies for all sites with ANGSD

ANGSD -b ${output}/${mypop}_bam.list \
-ref ${REF} -anc ${REF} \
-out ${output}/${mypop}_folded_allsites \
-nThreads 1 \
-remove_bads 1 \
-C 50 \
-baq 1 \
-minMapQ 20 \
-minQ 20 \
-setMinDepth 3 \
-minInd 2 \
-setMinDepthInd 1 \
-setMaxDepthInd 17 \
-skipTriallelic 1 \
-GL 1 \
-doCounts 1 \
-doMajorMinor 1 \
-doMaf 1 \
-doSaf 1 \
-fold 1

#Get a rough first estimate of the SFS and then use as a prior for the next estimate
realSFS ${output}/${mypop}_folded_allsites.saf.idx \
-maxIter 1000 -tole 1e-6 -P 1 \
> ${output}/${mypop}_outFold.sfs

# Get refined estimate of the SFS and do theta
ANGSD -b ${output}/${mypop}_bam.list \
-ref ${REF} -anc ${REF} \
-out ${output}/${mypop}_folded_allsites \
-nThreads 1 \
-remove_bads 1 \
-C 50 \
-baq 1 \
-minMapQ 20 \
-minQ 20 \
-setMinDepth 3 \
-minInd 2 \
-setMinDepthInd 1 \
-setMaxDepthInd 17 \
-skipTriallelic 1 \
-GL 1 \
-doCounts 1 \
-doMajorMinor 1 \
-doMaf 1 \
-doSaf 1 \
-fold 1 \
-pest ${output}/${mypop}_outFold.sfs \
-doThetas 1

#Use the doTheta output from above to estimate neucleotide diversity
thetaStat do_stat ${output}/${mypop}_folded_allsites.thetas.idx
```
## Use R to look at the mean variability in nucleotide diversity in the population XWS






------
<div id='id-section24'/>
### Entry 24: 2020-02-13, Thursday.



------
<div id='id-section25'/>
### Entry 25: 2020-02-14, Friday.



------
<div id='id-section26'/>
### Entry 26: 2020-02-17, Monday.



------
<div id='id-section27'/>
### Entry 27: 2020-02-18, Tuesday.



------
<div id='id-section28'/>
### Entry 28: 2020-02-19, Wednesday.
# [Population Genomics Day 4](https://pespenilab.github.io/Ecological-Genomics/2020-02-12_PopGenomics_Day4%20(1).html)

## Unfolded vs. Folded sfs
+ We noticed a pattern in the site frequency histogram where there were a lot of SNPs with high frequency.
  + we observed a quadratic curve instead of a asymtotic curve
  + This could be due to assuming that the reference genome was ancestral to the sampled genomes, when in reality it could have represented another variant not ancestral
  + When you can't be confident about the ancestry, you can fold!
  + when you can't be confident about the derived allele frequnecy, consider the minor allele frequency rather than the derived
    + no minor alleles means that the major allele is fixed. This is an alternative to ancestral
  + The folded spectra wraps the SFS around such that high frequency "derived" alleles are put into small bins (low minor allele frequency)




------
<div id='id-section29'/>
### Entry 29: 2020-02-20, Thursday.



------
<div id='id-section30'/>
### Entry 30: 2020-02-21, Friday.



------
<div id='id-section31'/>
### Entry 31: 2020-02-24, Monday.



------
<div id='id-section32'/>
### Entry 32: 2020-02-25, Tuesday.



------
<div id='id-section33'/>
### Entry 33: 2020-02-26, Wednesday.

# [Transcriptomics Day 1](https://pespenilab.github.io/Ecological-Genomics/2020-02-26_RNA-seq_Day1.html)
## Red Spruce data continued!
+ same species, different extraction/experiment
+ sometimes spruce is in warmer drier areas based on microclimate differences
+ This experiment samples genetic variation from sites that were polarized into cool & wet vs. hot & dry based on historic climate Data
+ assess gene expression
+ 5 families from hot and dry
+ 5 families from cool and wet
+ Treatments
  + control- watered daily
  + heat
  + heat and drought
+ Extracted RNA from whole seedlings.
  + anything strong will be seen anything tissue specific will be lost
  + collapse tissue variation
+ Nreps- no replicates from within a family
    + 5 families from each source climate regime
+ moms with cones on them were collected from nature and brought home and raised in a common garden experiment
  + if all the seeds came with things that are different to the site (eg: soil microbes, fungi etc) than we might have to sterilize them, which the student doing this did



## Library Prep
+ you don't need much sample for RNA seq
+ samples were sent to Cornell for 3' tag sequencing
+ samples were demultiplexed (sorted back to their population/organized) and named according to the convention: POP_FAM_TRT_DAY
  + TRT- treatment (control, heat, heat and drought)
  + good to keep these at two letters or numbers

## What questions can we ask with these experimental designs with these data
+ *What are our factors to consider in building these questions:*
  + Treatment- control, heat, heat drought
  + Source Climate- HotDry, CoolWet
  + Time- 0, 5, 10 (days on which they were sampled)
+ *Do individuals from different Climate Sources have different gene expression
considering:*
  1. each condition (expression climate source as main effect and treatment)
    + does response to treatment condition depend on source climate (everytime you say "depend on" it's an interaction)
  2. time points (interaction between source climate and time)
  3. masking any differences between source and climate, we could compare treatment and time
  4. Which specific genes are involed in these reponses, do they reveal function enrichment of particular pathways?
  5. Do differentially expressed genes or pathways show evidence of positive selection (compare DE genes with popgen signitures of selection)
  6. Can we use association mapping to identify eQTL associated with DE responses?

## Pipeline
+ Trimmomatic is done!
  + cut off first 12 bases across all reads (hexomers)
+ We're using the white spruce as a reference transcriptome
+ Do fastqc on raw and cleaned reads
+ Mapping process
  + Map reads while simultaneously quantifying abundance using Salmon
  + Salmon makes a quantSF file (one for everyone in your data set)
  + import these into R package called DESeq2 for analysis and visualization
    + also use DESeq2 to do statistical tests for differential gene expression
+ I am going to be working with BRU D and H

## fastqc
+ edit using vim
+ copy the fashtq.sh script and rename it BRU.sh to edit it for this purpose
+ `cp fastqc.sh BRU.sh `
+ edit it to include my population name

``````
#!/bin/bash

#this directory is using bash code

cd ~/EcologicalGenomics/myresults/

mkdir transfastqc #I am creating a new directory

for file in data /data/project_data/RS_RNASeq/fastq/BRU*H*fastq.gz

do

fastqc ${file} -o transfastqc/

#the lower case o means dont spit it to this directory put it in the folder i tell you to be in. only need to give the portion of the path that is beyond where you are.

done
``````

+ Then do this again on your cleaned reads
  + `/data/project_data/RS_RNASeq/fastq/cleanreads/BRU*H*cl.fq`
  + `cp BRU.sh cleanBRU.sh`
  + then `vim cleanBRU.sh `
  + then I edited this to look like this

 ``````
  #!/bin/bash

  #this directory is using bash code

  cd ~/EcologicalGenomics/myresults/

  mkdir cleanedtransfastqc #I am creating a new directory to store these cleaned reads files

  for file in data /data/project_data/RS_RNASeq/fastq/cleanreads/BRU*H*.cl.fq

  do

  fastqc ${file} -o cleanedtransfastqc/

  #the lower case o means dont spit it to this directory put it in the folder i tell you to be in. only need to give the portion of the path that is beyond where you are.

  done
``````
+ comparing the cleaned vs uncleaned
  + GS drops
  + number of bases drop
  + Per seq GC content- you can see a strong signature of first 12 bases
    + this is cone after cutting
  + Per sequence GC content












------
<div id='id-section34'/>
### Entry 34: 2020-02-27, Thursday.



------
<div id='id-section35'/>
### Entry 35: 2020-02-28, Friday.



------
<div id='id-section36'/>
### Entry 36: 2020-03-02, Monday.

# [Differential Gene Expression](https://tsoleary.github.io/eco_genomics/deg_info_update.html)
### What is it?
+ looking at difference in transcript abundance between experimental groups
  + which genes are changing, are any pathways enriched, why are they changing
  + if were primariliy interested in how geneotypes are related to phenotypes, we can run experiments that manipulated whether phenotypes are a product of geneotype and environment, we can control the environment and get at the geneotype
  #### How did we get here?
  + sample Prep
    + collect samples
    + RNA extraction
    + library Prep
    + sequencing
    + quality control -> FastQC, trimming, visualize again
    + map genes onto the genomes
    + Counting- how does the transcriptome look- what maps

### Normalization
+ data matrix/genecount table
| sample1  |  sample2 |  sample3 |   |   |
|---|---|---|---|---|
| 120  | 60  |  22 |   |   |
|  50 |  25 |  9 |
|  75 |  37 | 14
0  | 0  |  200

    + gene 4 is highly expressed only in sample3
    + the above example, each row is a gene
#### Normalization methods
  + Counts per million (CPM) - depth
  + transcripts per kilobase million
  + fragments per kilobase million
    + second two both account for depth and gene length
    + if you want to make a comparision between two genes within a sample, you would want to account for gene length
  + edge R- accounts for depth, gene size, and library composition
  + DESeq2 - accounts for dept composition
      + log transformed values
      + get a gene wide average
      + youll end up knowing the genes that aren't really changing
      + somehow you get a scaling factor- (log transformed values, take a rowise average. then take a ratio of these values, then a median of the ratios)
        + for the example above these are 2.23, 1.11, and 0.41
        + divide the values in the table by the scaling factor
        + this allows for a more realistic view of expression

### Differential Expression
+ 20,000 * 5% = 1,000 false positive
+ false negative- two normal distributions that aren't really overlapping  
    + you happen to pick some within this small overlap and call them non differentially expressed genes
+ Independent Filtering
    + limiting the number of tests based on a cutoff
        + base mean
        + DESeq2 tosses an NA in the even that something is filtered out
    + Benjamin Hochberg (BH)
      + adjusted pvalue
        + control false positives
        + FDR- false discovery rate

### Statistical Models
+ how well do these variables predict expression levels
+ if they do predict them, we will see a low pvalue

### Visualization
+ look at quality control metrics
+ sequencinging summary statistics
  + make barplots that are informative of a total read counts (per sample)
  + can also visualize null read counts (eg 0s in the above chart, but one sample was super high)
  + or visualize seqencing after normalization and hopefully the medians are in roughly the sample spot (in a boxplot)
+ PCA hopefully you observe good clustering of experimental groups
  + instead of tight groups, you might have two groups similarly
+ heat maps
  + each gene corresponds to a row
  + colored based on log fold change how much theyre changing relative to each other
  + sometimes this will be combined with hierarchical clustering
    + this will group samples that are behaving similarly to each other based on how much the genes are changing
+ ven diagrams
  + can split with upregulated or down regulated genes
+ MA plot
  + log of mean expression on the x axis
  + log of relative fold change on the y axis
    + some people also use a fold change cut off
      + this would be pictured on the MA plot
+ Volcano plot
  + log pvalue on y axis
  + log fold change on x axis
+ log fold change scattered plot
  + LFC of condition 2 vs control
  + LVF condition 1 vs control
    + shared things along the line and then unique things are in the quandrants and line
+ individual gene treatment response curve (thomas made this nanme up)
  + y axis relative fold change (or log fold change)
  + treatments on x axis

### Enrichment Analysis
+ look at gene sets and compare your differentially expressed genes to these gene sets
+ functional data bases
  + gene ontology (GO) is common
    + has subcategories: molecular function (eg: atc cyclase activity), biological process (eg pyridimine biosynthesis), or cellular compartement (where in the cell, what molecular macro molecule is involved, eg: ribosome)
  + KEGG pathway
    +genetic information processing
      + transcription
        + splicesomal process
+ methods:
  + over representation analysis
    + looking to see if gene set is more than by random chance represented in differentially expressed groups
      + based on the difference in representation between one gene set compared to all the differentially expressed genes
    + requires a cut off
  + Gene set enrichment analysis
    + rank based method
    + usually log fold change
    + are genes within the set clustered towards one end of the ranked list
    + if you had a bunch of genes in a set and they were all ranked high, they would be considered enriched
    + doesnt require a cut off so you are able to retain more data  









------
<div id='id-section37'/>
### Entry 37: 2020-03-03, Tuesday.



------
<div id='id-section38'/>
### Entry 38: 2020-03-04, Wednesday.

# [Transcriptomics Day 2](https://pespenilab.github.io/Ecological-Genomics/2020-03-044_RNA-seq_Day2.html)

## Quantifying and mapping our reads using Salmon

`salmon quant -i <your index here> -l A -r reads.fq --validateMappings -o transcripts_quant`

+ ## Write a script to map our single read samples
  + I did this by copying the cleanBRU.sh file and renaming it salmon, then editing it

```
#!/bin/bash

#this directory is using bash code

cd /data/project_data/RS_RNASeq/fastq/cleanreads


for file in BRU*H*.cl.fq

do

        echo "starting sample ${file}"
        salmon quant -i /data/project_data/RS_RNASeq/ReferenceTranscriptome/Pabies_HC27_index -l A -r ${file} --validateMappings -o /data/project_data/RS_RNASeq/salmon/cleanedreads/${file}

# all of our data are going to the same folder

done


```
  + reviewing challenges:
  + we cd to the directory where we knew the files were
  + the echo just means that it is telling you when it is starting a file

+ ## Check mapping rate
  + we mapped to a high quality reference genome, lowered kmer parameter to allow for mapping smaller fragments
  + cd `cd /data/project_data/RS_RNASeq/salmon/cleanedreads`
  + `grep -r --include \*.log -e 'Mapping rate'`
  + this command shows the percent reads in each file that mapped to the reference transcriptome with high confidence
    + high confidence is based on homology (~70%) from BLAST between red and norway spruce
      + if they're conifer specific genes they wont be in high confidence because they might not map to any more distantly related (non conifer, other plant)
    + our mapping rate isn't very good (~20%)
      + there are multiple reasons that this could be wrong
        + we could remap to another kmer
  + To get the higher mapping rate we changed the script:

```
#!/bin/bash

#this directory is using bash code

cd /data/project_data/RS_RNASeq/fastq/cleanreads/


for file in BRU*H*.cl.fq

do

        echo "starting sample ${file}"
        salmon quant -i /data/project_data/RS_RNASeq/ReferenceTranscriptome/Pabies_cds_index -l A -r ${file} --validateMappings --seqBias -o /data/project_data/RS_RNASeq/salmon/allmapping/${file}

# all of our data are going to the same folder

done
```
  + in this script we use all the references not just the high quality maps
  + this script uses a different kmer flag
  + this allows there to be a slightly worse map between the reference and the read
  + now these data are located here `cd /data/project_data/RS_RNASeq/salmon/allmapping/`
    + ll shows all the Files
    + I copied one of mine and entered it: `cd BRU_05_H_10_TTCCGC_R1.cl.fq`
    + then did `head.quant.sf`
      + this shows the first 10 lines of the file
    + I also did `grep -r --include \*.log -e 'Mapping rate'`
      + mapping rate for that specific file is ~39% which is much higher!

In R:
```
library(tximportData)
library(tximport)

#locate the directory containing the files.
dir <- "/data/project_data/RS_RNASeq/salmon/"
list.files(dir) # lists our files and shows that we're in the right place

# read in table with sample ids
# lines up sample names with quantsf files in their respective directories
samples <- read.table("/data/project_data/RS_RNASeq/salmon/RS_samples.txt", header=TRUE)

# now point to quant files
# the all files function uses column sample in the file called samples
# finds the quant sf file in the Directory
all_files <- file.path(dir, samples$sample, "quant.sf")
names(all_files) <- samples$sample

# what would be used if linked transcripts to genes
#txi <- tximport(files, type = "salmon", tx2gene = tx2gene)
# to be able to run without tx2gene
txi <- tximport(all_files, type = "salmon", txOut=TRUE)
# the above import using tximport. this has an option to collapse transcripts into genes. If you have splice variants this just counts reads that mapped to genes. But ours were already all genes (so txOut=TRUE)!
names(txi)

head(txi$counts)

countsMatrix <- txi$counts
dim(countsMatrix)
#[1] 66069    76

# To write out
write.table(countsMatrix, file = "RS_countsMatrix.txt", col.names = T, row.names = T, quote = F)

------
<div id='id-section39'/>
### Entry 39: 2020-03-05, Thursday.



------
<div id='id-section40'/>
### Entry 40: 2020-03-06, Friday.



------
<div id='id-section41'/>
### Entry 41: 2020-03-09, Monday.



------
<div id='id-section42'/>
### Entry 42: 2020-03-10, Tuesday.



------
<div id='id-section43'/>
### Entry 43: 2020-03-11, Wednesday.



------
<div id='id-section44'/>
### Entry 44: 2020-03-12, Thursday.



------
<div id='id-section45'/>
### Entry 45: 2020-03-13, Friday.



------
<div id='id-section46'/>
### Entry 46: 2020-03-16, Monday.



------
<div id='id-section47'/>
### Entry 47: 2020-03-17, Tuesday.



------
<div id='id-section48'/>
### Entry 48: 2020-03-18, Wednesday.



------
<div id='id-section49'/>
### Entry 49: 2020-03-19, Thursday.



------
<div id='id-section50'/>
### Entry 50: 2020-03-20, Friday.



------
<div id='id-section51'/>
### Entry 51: 2020-03-23, Monday.



------
<div id='id-section52'/>
### Entry 52: 2020-03-24, Tuesday.



------
<div id='id-section53'/>
### Entry 53: 2020-03-25, Wednesday.



------
<div id='id-section54'/>
### Entry 54: 2020-03-26, Thursday.



------
<div id='id-section55'/>
### Entry 55: 2020-03-27, Friday.



------
<div id='id-section56'/>
### Entry 56: 2020-03-30, Monday.



------
<div id='id-section57'/>
### Entry 57: 2020-03-31, Tuesday.



------
<div id='id-section58'/>
### Entry 58: 2020-04-01, Wednesday.



------
<div id='id-section59'/>
### Entry 59: 2020-04-02, Thursday.



------
<div id='id-section60'/>
### Entry 60: 2020-04-03, Friday.



------
<div id='id-section61'/>
### Entry 61: 2020-04-06, Monday.



------
<div id='id-section62'/>
### Entry 62: 2020-04-07, Tuesday.



------
<div id='id-section63'/>
### Entry 63: 2020-04-08, Wednesday.



------
<div id='id-section64'/>
### Entry 64: 2020-04-09, Thursday.



------
<div id='id-section65'/>
### Entry 65: 2020-04-10, Friday.



------
<div id='id-section66'/>
### Entry 66: 2020-04-13, Monday.



------
<div id='id-section67'/>
### Entry 67: 2020-04-14, Tuesday.



------
<div id='id-section68'/>
### Entry 68: 2020-04-15, Wednesday.



------
<div id='id-section69'/>
### Entry 69: 2020-04-16, Thursday.



------
<div id='id-section70'/>
### Entry 70: 2020-04-17, Friday.



------
<div id='id-section71'/>
### Entry 71: 2020-04-20, Monday.



------
<div id='id-section72'/>
### Entry 72: 2020-04-21, Tuesday.



------
<div id='id-section73'/>
### Entry 73: 2020-04-22, Wednesday.



------
<div id='id-section74'/>
### Entry 74: 2020-04-23, Thursday.



------
<div id='id-section75'/>
### Entry 75: 2020-04-24, Friday.



------
<div id='id-section76'/>
### Entry 76: 2020-04-27, Monday.



------
<div id='id-section77'/>
### Entry 77: 2020-04-28, Tuesday.



------
<div id='id-section78'/>
### Entry 78: 2020-04-29, Wednesday.



------
<div id='id-section79'/>
### Entry 79: 2020-04-30, Thursday.



------
<div id='id-section80'/>
### Entry 80: 2020-05-01, Friday.



------
<div id='id-section81'/>
### Entry 81: 2020-05-04, Monday.



------
<div id='id-section82'/>
### Entry 82: 2020-05-05, Tuesday.



------
<div id='id-section83'/>
### Entry 83: 2020-05-06, Wednesday.



------
<div id='id-section84'/>
### Entry 84: 2020-05-07, Thursday.



------
<div id='id-section85'/>
### Entry 85: 2020-05-08, Friday.
